import os
import logging
import pandas as pd
from datetime import datetime
import click
from datasets import load_dataset
# from rebuff import Rebuff
from src.classifiers.random_forest_classifier import RandomForestClassifier
from src.classifiers.distill_bert_classifier import DistilBERTClassifier
from src.classifiers.features.feature_extractor import FeatureExtractor

log_dir = os.path.join(os.getcwd(), 'logs')
os.makedirs(log_dir, exist_ok=True)
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
log_file = os.path.join(log_dir, f'injection_detection_run_{timestamp}.log')
logging.basicConfig(
    format='%(asctime)s - %(levelname)s - %(message)s',
    level=logging.INFO,
    datefmt='%Y-%m-%d %H:%M:%S',
    handlers=[
        logging.FileHandler(log_file),
        logging.StreamHandler()
    ]
)

def load_datasets():
    """Load HuggingFace dataset and custom training data"""
    logging.info("Loading datasets...")
    
    # Load HuggingFace dataset
    hf_dataset = load_dataset("deepset/prompt-injections")
    hf_train_df = pd.DataFrame(hf_dataset['train'])
    test_df = pd.DataFrame(hf_dataset['test'])
    
    # Load custom training dataset
    custom_train_df = pd.read_csv(os.path.join("src","dataset_generation","dataset", "custom", "prompts", "train.csv"))
    
    # Combine training sets
    train_df = pd.concat([hf_train_df, custom_train_df], ignore_index=True)
    
    logging.info(f"Combined training set size: {len(train_df)}")
    logging.info(f"Test set size: {len(test_df)}")
    
    return train_df, test_df, custom_train_df

def balance_dataset(train_df, custom_train_df):
    """Balance the training dataset using custom examples"""
    logging.info("Checking and balancing dataset...")
    
    # Calculate class distribution
    class_counts = train_df['label'].value_counts()
    majority_class = class_counts.index[0]
    minority_class = class_counts.index[1]
    imbalance_ratio = class_counts[majority_class] / class_counts[minority_class]
    
    logging.info(f"Initial class distribution - Class 0: {class_counts[0]}, Class 1: {class_counts[1]}")
    logging.info(f"Imbalance ratio: {imbalance_ratio:.2f}")
    
    if imbalance_ratio > 1.2:  # If dataset is imbalanced (20% threshold)
        logging.info("Dataset is imbalanced. Augmenting with custom examples...")
        
        # Get examples needed from minority class
        samples_needed = class_counts[majority_class] - class_counts[minority_class]
        
        # Get additional samples from custom dataset
        custom_minority_samples = custom_train_df[custom_train_df['label'] == minority_class]
        
        if len(custom_minority_samples) > 0:
            # If we have enough samples, use them directly
            if len(custom_minority_samples) >= samples_needed:
                additional_samples = custom_minority_samples.sample(n=samples_needed, replace=False)
            # If we don't have enough, use what we have with replacement
            else:
                additional_samples = custom_minority_samples.sample(n=samples_needed, replace=True)
            
            # Add the new samples to the training set
            train_df = pd.concat([train_df, additional_samples], ignore_index=True)
            
            logging.info(f"Added {len(additional_samples)} samples from custom dataset")
            logging.info(f"New class distribution - Class 0: {len(train_df[train_df['label'] == 0])}, "
                        f"Class 1: {len(train_df[train_df['label'] == 1])}")
        else:
            logging.warning("No suitable samples found in custom dataset for balancing")
    
    return train_df

# Rebuff API for prompt injection detection - for labelling prompts
# https://blog.langchain.dev/rebuff/
# rb = Rebuff(api_token="your_rebuff_api_token_here", api_url="https://alpha.rebuff.ai")

@click.command()
@click.option('--pdfs', type=click.BOOL, default=False, help="Enable PDF analysis (True/False)")
@click.option('--classifier_type', type=click.Choice(['randomforest', 'distillbert'], case_sensitive=False), 
              default='randomforest', help="Type of classifier to use")
def main(pdfs, classifier_type):
    start_time = datetime.now()

    # Load datasets
    train_df, test_df, custom_train_df = load_datasets()
    
    # Balance dataset if needed using custom training data
    train_df = balance_dataset(train_df, custom_train_df)

    # Check if the training set is balanced
    injection_count = train_df['label'].value_counts()
    total_train = len(train_df)
    logging.info(f"Training set balance: {'Balanced' if abs(injection_count[0] - injection_count[1]) / total_train < 0.2 else 'Imbalanced'}")

    # Check for unlabeled data (using Rebuff to detect injection threats)
    # https://blog.langchain.dev/rebuff/
    #detection_metrics, is_injection = rb.detect_injection(df['text'])
    #df.at[idx, 'label'] = 1 if is_injection else 0

    for df in [train_df, test_df]:
        df['length'] = df['text'].str.len()
        df['word_count'] = df['text'].str.split().str.len()
    
    # initialize feature extractor and classifier
    feature_extractor = FeatureExtractor()
    classifier = RandomForestClassifier() if classifier_type == "randomforest" else DistilBERTClassifier()
    
    if classifier_type == "randomforest":
        # Random Forest requires feature engineering as it will help the model to learn better
        feature_extractor = FeatureExtractor()
        train_features = [feature_extractor.extract_features(text) for text in train_df['text']]
        test_features = [feature_extractor.extract_features(text) for text in test_df['text']]
        
        classifier.fit(train_features, train_df['label'].tolist())
        metrics = classifier.evaluate(test_features, test_df['label'].tolist())
    else:
        # Transformer models like DistilBERT don't need feature engineering as they can work directly with raw text, creating embeddings internally that act as features
        classifier.fit(train_df['text'].tolist(), train_df['label'].tolist())
        metrics = classifier.evaluate(test_df['text'].tolist(), test_df['label'].tolist())
    
    print("\n=== Dataset Information ===")
    print(f"Training set: {len(train_df)} samples ({train_df['label'].sum()} injections, {train_df['label'].mean()*100:.1f}%)")
    print(f"Test set: {len(test_df)} samples ({test_df['label'].sum()} injections, {test_df['label'].mean()*100:.1f}%)")
    
    print("\n=== Model Performance ===")
    print(f"Accuracy: {metrics['accuracy']:.4f}")
    print("\nConfusion Matrix:")
    print(f"True Positives: {metrics['true_positives']} | False Positives: {metrics['false_positives']}")
    print(f"False Negatives: {metrics['false_negatives']} | True Negatives: {metrics['true_negatives']}")
    
    print(f"\nTotal runtime: {datetime.now() - start_time}")

if __name__ == "__main__":
    main()