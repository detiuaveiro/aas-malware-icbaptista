import os
import logging
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from typing import Tuple
from datetime import datetime
from datasets import load_dataset
from src.detector.injection_detector import InjectionDetector

# Configure logging
log_dir = os.path.join(os.getcwd(), 'logs')
os.makedirs(log_dir, exist_ok=True)
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
log_file = os.path.join(log_dir, f'injection_detection_run_{timestamp}.log')
logging.basicConfig(
    format='%(asctime)s - %(levelname)s - %(message)s',
    level=logging.INFO,
    datefmt='%Y-%m-%d %H:%M:%S',
    handlers=[
        logging.FileHandler(log_file),
        logging.StreamHandler()  
    ]
)

class InjectionDataLoader:
   def __init__(self):
       self.train_df = None
       self.test_df = None
       
   def load_data(self) -> Tuple[pd.DataFrame, pd.DataFrame]:
       logging.info("Loading datasets from HuggingFace...")
       dataset = load_dataset("deepset/prompt-injections")
       self.train_df = pd.DataFrame(dataset['train'])
       self.test_df = pd.DataFrame(dataset['test'])
       logging.info(f"Loaded {len(self.train_df)} training and {len(self.test_df)} test samples")
       return self.train_df, self.test_df
   
   def preprocess_data(self) -> Tuple[pd.DataFrame, pd.DataFrame]:
       logging.info("Preprocessing datasets...")
       for df in [self.train_df, self.test_df]:
           df['label'] = (df['label'] == 'INJECTION').astype(int)
           df['length'] = df['text'].str.len()
           df['word_count'] = df['text'].str.split().str.len()
       logging.info("Preprocessing completed")
       return self.train_df, self.test_df

def main():
   start_time = datetime.now()
   logging.info(f"Starting prompt injection detection system at {start_time}")
   
   loader = InjectionDataLoader()
   detector = InjectionDetector(classifier_type="randomforest")
   
   train_df, test_df = loader.load_data()
   train_df, test_df = loader.preprocess_data()
   
   # Count actual injections
   train_injections = train_df['label'].sum()
   test_injections = test_df['label'].sum() 
   
   logging.info("\nActual Injection Counts:")
   logging.info(f"Training set: {train_injections} injections out of {len(train_df)} samples ({train_injections/len(train_df)*100:.2f}%)")
   logging.info(f"Testing set: {test_injections} injections out of {len(test_df)} samples ({test_injections/len(test_df)*100:.2f}%)")
   
   logging.info("\nDataset Statistics:")
   logging.info(f"Training set: {len(train_df)} samples ({train_df['label'].mean()*100:.1f}% injections)")
   logging.info(f"Test set: {len(test_df)} samples ({test_df['label'].mean()*100:.1f}% injections)")
   
   tp, fp, tn, fn = 0, 0, 0, 0
   detected_injections = 0
   
   logging.info("\nAnalyzing test set prompts...")
   for i, row in test_df.iterrows():
       result = detector.analyze_prompt(row['text'])
       predicted = 1 if result['prediction'] == 'malicious' else 0
       
       if predicted == 1:
           detected_injections += 1
           
       if predicted == 1 and row['label'] == 1:
           tp += 1
       elif predicted == 1 and row['label'] == 0:
           fp += 1
       elif predicted == 0 and row['label'] == 0:
           tn += 1
       else:
           fn += 1
           
       if result['risk_score'] > 0.5:
           logging.info(f"\nHigh Risk Prompt {i}:")
           logging.info(f"Text: {row['text']}")
           logging.info(f"True label: {'INJECTION' if row['label'] == 1 else 'BENIGN'}")
           logging.info(f"Prediction: {result['prediction']}")
           logging.info(f"Risk score: {result['risk_score']:.4f}")
           logging.info(f"Risk level: {result['risk_level']}")
           logging.info("Features:")
           for feature, value in result['features'].items():
               logging.info(f"- {feature}: {value}")
           logging.info("Analysis:")
           for category, details in result['analysis'].items():
               logging.info(f"- {category}: {details}")
           if result['analysis']['risk_factors']:
               logging.info("Risk factors:")
               for factor in result['analysis']['risk_factors']:
                   logging.info(f"- Type: {factor['type']}")
                   logging.info(f"  Severity: {factor['severity']}")
                   logging.info(f"  Details: {factor['details']}")
                   if 'matches' in factor:
                       logging.info(f"  Matches: {factor['matches']}")
   
   logging.info(f"\nDetected {detected_injections} injections in test set ({detected_injections/len(test_df)*100:.2f}%)")
   
   total = len(test_df)
   accuracy = (tp + tn) / total
   precision = tp / (tp + fp) if (tp + fp) > 0 else 0
   recall = tp / (tp + fn) if (tp + fn) > 0 else 0
   f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
   
   logging.info("\nConfusion Matrix:")
   logging.info(f"True Positives: {tp} | False Positives: {fp}")
   logging.info(f"False Negatives: {fn} | True Negatives: {tn}")
   logging.info(f"\nMetrics:")
   logging.info(f"Accuracy: {accuracy:.4f}")
   logging.info(f"Precision: {precision:.4f}")
   logging.info(f"Recall: {recall:.4f}")
   logging.info(f"F1 Score: {f1:.4f}")
   
   cm = np.array([[tn, fp], [fn, tp]])
   plt.figure(figsize=(8, 6))
   sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
               xticklabels=['Benign', 'Malicious'],
               yticklabels=['Benign', 'Malicious'])
   plt.title('Confusion Matrix')
   plt.ylabel('True Label')
   plt.xlabel('Predicted Label')
   plt.savefig('confusion_matrix.png')
   plt.close()
   
   end_time = datetime.now()
   logging.info(f"\nAnalysis completed. Total runtime: {end_time - start_time}")

if __name__ == "__main__":
   main()