import os
import logging
import pandas as pd
from datetime import datetime
import click
from datasets import load_dataset
# from rebuff import Rebuff
from src.classifiers.random_forest_classifier import RandomForestClassifier
from src.classifiers.distill_bert_classifier import DistilBERTClassifier
from src.classifiers.features.feature_extractor import FeatureExtractor

log_dir = os.path.join(os.getcwd(), 'logs')
os.makedirs(log_dir, exist_ok=True)
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
log_file = os.path.join(log_dir, f'injection_detection_run_{timestamp}.log')
logging.basicConfig(
    format='%(asctime)s - %(levelname)s - %(message)s',
    level=logging.INFO,
    datefmt='%Y-%m-%d %H:%M:%S',
    handlers=[
        logging.FileHandler(log_file),
        logging.StreamHandler()
    ]
)

# Rebuff API for prompt injection detection - for labelling prompts
# https://blog.langchain.dev/rebuff/
# rb = Rebuff(api_token="your_rebuff_api_token_here", api_url="https://alpha.rebuff.ai")

@click.command()
@click.option('--dataset_source', type=click.Choice(['HuggingFace', 'custom'], case_sensitive=False), 
              required=True, help="Source of the dataset: HuggingFace or custom")
@click.option('--pdfs', type=click.BOOL, default=False, help="Enable PDF analysis (True/False)")
@click.option('--classifier_type', type=click.Choice(['randomforest', 'distillbert'], case_sensitive=False), 
              default='randomforest', help="Type of classifier to use")
def main(dataset_source, pdfs, classifier_type):
    start_time = datetime.now()

    # Load data 
    if dataset_source == "HuggingFace":
        dataset = load_dataset("deepset/prompt-injections")
        train_df = pd.DataFrame(dataset['train'])
        test_df = pd.DataFrame(dataset['test'])
    else:  # custom
        train_df = pd.read_csv(os.path.join("dataset", "custom", "prompts", "train.csv"))
        test_df = pd.read_csv(os.path.join("dataset", "custom", "prompts", "test.csv"))

    # Check if the training set is balanced
    injection_count = train_df['label'].value_counts()
    total_train = len(train_df)
    logging.info(f"Training set balance: {'Balanced' if abs(injection_count[0] - injection_count[1]) / total_train < 0.2 else 'Imbalanced'}")

    # Check for unlabeled data (using Rebuff to detect injection threats)
    # https://blog.langchain.dev/rebuff/
    #detection_metrics, is_injection = rb.detect_injection(df['text'])
    #df.at[idx, 'label'] = 1 if is_injection else 0

    for df in [train_df, test_df]:
        df['length'] = df['text'].str.len()
        df['word_count'] = df['text'].str.split().str.len()
    
    # initialize feature extractor and classifier
    feature_extractor = FeatureExtractor()
    classifier = RandomForestClassifier() if classifier_type == "randomforest" else DistilBERTClassifier()
    
    if classifier_type == "randomforest":
        # Random Forest requires feature engineering as it will help the model to learn better
        feature_extractor = FeatureExtractor()
        train_features = [feature_extractor.extract_features(text) for text in train_df['text']]
        test_features = [feature_extractor.extract_features(text) for text in test_df['text']]
        
        classifier.fit(train_features, train_df['label'].tolist())
        metrics = classifier.evaluate(test_features, test_df['label'].tolist())
    else:
        # Transformer models like DistilBERT don't need feature engineering as they can work directly with raw text, creating embeddings internally that act as features
        classifier.fit(train_df['text'].tolist(), train_df['label'].tolist())
        metrics = classifier.evaluate(test_df['text'].tolist(), test_df['label'].tolist())
    
    print("\n=== Dataset Information ===")
    print(f"Training set: {len(train_df)} samples ({train_df['label'].sum()} injections, {train_df['label'].mean()*100:.1f}%)")
    print(f"Test set: {len(test_df)} samples ({test_df['label'].sum()} injections, {test_df['label'].mean()*100:.1f}%)")
    
    print("\n=== Model Performance ===")
    print(f"Accuracy: {metrics['accuracy']:.4f}")
    print("\nConfusion Matrix:")
    print(f"True Positives: {metrics['true_positives']} | False Positives: {metrics['false_positives']}")
    print(f"False Negatives: {metrics['false_negatives']} | True Negatives: {metrics['true_negatives']}")
    
    print(f"\nTotal runtime: {datetime.now() - start_time}")

if __name__ == "__main__":
    main()