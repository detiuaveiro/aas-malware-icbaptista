import logging
from typing import Dict, Tuple, List
import torch
from torch.utils.data import DataLoader, Dataset
from src.classifiers.base_classifier import BaseClassifier
from tqdm import tqdm
from transformers import DistilBertForSequenceClassification, DistilBertTokenizer

class TextDataset(Dataset):
    """
    Custom PyTorch Dataset for text classification. Allows PyTorch to efficiently load and process data. 
    Preprocessing of the prompts with the tokenizer. 
    """
    def __init__(self, texts, labels, tokenizer, max_length: int = 128):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length  # Add this line

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        label = self.labels[idx]
        
        # Tokenization 
        encoding = self.tokenizer(
            text, 
            truncation=True, 
            padding='max_length',  
            max_length=self.max_length, 
            return_tensors='pt'
        )
        
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long)
        }

class DistilBERTClassifier(BaseClassifier):
    """DistilBERT-based classifier implementation"""
    
    def __init__(self, model_name='distilbert-base-uncased'):
        logging.info(f"Initializing DistilBERT classifier: {model_name}")
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.tokenizer = DistilBertTokenizer.from_pretrained(model_name)
        self.model = DistilBertForSequenceClassification.from_pretrained(
            model_name,
            num_labels=2  # Binary classification
        ).to(self.device)

    def fit(self, texts: List[str], labels: List[int], 
            batch_size: int = 16, epochs: int = 4, learning_rate: float = 2e-5):
        """
        Train the model on the provided dataset.
        
        Args:
            features (List[Dict]): List of feature dictionaries
            labels (List[int]): List of labels (0 = benign, 1 = malicious)
            batch_size (int): Training batch size
            epochs (int): Number of training epochs
            learning_rate (float): Learning rate for optimization
        """
        logging.info("Starting DistilBERT training...")
        
        # Prepare training data
        train_dataset = TextDataset(texts, labels, self.tokenizer)
        train_loader = DataLoader(
            train_dataset,
            batch_size=batch_size,
            shuffle=True
        )
        
        optimizer = torch.optim.AdamW(self.model.parameters(), lr=learning_rate)
        
        # Training loop
        self.model.train()
        for epoch in range(epochs):
            total_loss = 0
            correct_predictions = 0
            total_predictions = 0
            
            for batch in tqdm(train_loader, desc=f"Epoch {epoch + 1}/{epochs}"):
                optimizer.zero_grad()
                
                # Move batch to device
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                labels = batch['labels'].to(self.device)
                
                # Forward pass
                outputs = self.model(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    labels=labels
                )
                
                loss = outputs.loss
                total_loss += loss.item()
                
                # Calculate accuracy
                predictions = torch.argmax(outputs.logits, dim=1)
                correct_predictions += (predictions == labels).sum().item()
                total_predictions += labels.size(0)
                
                # Backward pass
                loss.backward()
                optimizer.step()
            
            # Log epoch metrics
            avg_loss = total_loss / len(train_loader)
            accuracy = correct_predictions / total_predictions
            logging.info(f"Epoch {epoch + 1}/{epochs}, "
                        f"Average Loss: {avg_loss:.4f}, "
                        f"Accuracy: {accuracy*100:.2f}%")
        
        logging.info("Training completed")
    
    def predict(self, features: Dict) -> Tuple[float, str]:
        """
        Predict using DistilBERT model.
        
        Args:
            features (Dict): Input features with text
            
        Returns:
            Tuple[float, str]: (risk_score, prediction)
        """
        self.model.eval()
        
        # Tokenization
        print(features)
        inputs = self.tokenizer(
            features,
            return_tensors='pt',
            truncation=True,
            padding=True,
            max_length=128
        )
        
        input_ids = inputs['input_ids'].to(self.device)
        attention_mask = inputs['attention_mask'].to(self.device)
        
        # Get prediction
        with torch.no_grad():
            outputs = self.model(
                input_ids=input_ids,
                attention_mask=attention_mask
            )
            probabilities = torch.softmax(outputs.logits, dim=1)
            risk_score = probabilities[0][1].item()  # Probability of malicious class
            prediction = "malicious" if risk_score > 0.5 else "benign"
            
        logging.info(f"DistilBERT prediction: {prediction} (score: {risk_score:.4f})")
        return risk_score, prediction