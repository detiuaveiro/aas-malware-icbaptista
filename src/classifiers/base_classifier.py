from typing import Dict, Tuple, List
from abc import ABC, abstractmethod
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import logging
import json
import os
import seaborn as sns
import matplotlib.pyplot as plt
from datetime import datetime

class BaseClassifier(ABC):
    """Base class for injection classifiers"""
    
    def __init__(self):
        self.model = None
        
    @abstractmethod
    def fit(self, features: List[Dict], labels: List[int]) -> None:
        """Train the classifier"""
        pass

    @abstractmethod
    def predict(self, features: Dict) -> Tuple[float, str]:
        """Make prediction"""
        pass

    def evaluate(self, test_features: List[Dict], test_labels: List[int]) -> Dict:
        """
        Evaluate model performance.
        
        Args:
            test_features: List of feature dictionaries
            test_labels: Ground truth labels
            
        Returns:
            Dict containing metrics including precision, recall, and F1-score
        """
        predictions = []
        scores = []
        
        for features in test_features:
            score, pred = self.predict(features)
            scores.append(score)
            predictions.append(1 if pred == "malicious" else 0)
            
        # Calculate metrics
        conf_matrix = confusion_matrix(test_labels, predictions)
        
        # Get classification report with detailed metrics
        class_report_dict = classification_report(
            test_labels, 
            predictions, 
            target_names=['benign', 'malicious'], 
            output_dict=True
        )
        
        # Calculate precision, recall, and F1 for malicious class (positive class)
        precision = class_report_dict['malicious']['precision']
        recall = class_report_dict['malicious']['recall']
        f1 = class_report_dict['malicious']['f1-score']
        
        metrics = {
            'accuracy': accuracy_score(test_labels, predictions),
            'precision': precision,
            'recall': recall,
            'f1_score': f1,
            'confusion_matrix': conf_matrix.tolist(),
            'classification_report': class_report_dict,
            'false_positives': int(conf_matrix[0][1]),
            'false_negatives': int(conf_matrix[1][0]),
            'true_positives': int(conf_matrix[1][1]),
            'true_negatives': int(conf_matrix[0][0])
        }

        # Directory to store results
        results_dir = "results"
        os.makedirs(results_dir, exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # Plot confusion matrix
        plt.figure(figsize=(8, 6))
        sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
                    xticklabels=['Benign', 'Malicious'],
                    yticklabels=['Benign', 'Malicious'])
        plt.title('Confusion Matrix')
        plt.ylabel('True Label')
        plt.xlabel('Predicted Label')

        # Save the confusion matrix plot
        plt.savefig(os.path.join(results_dir, f'confusion_matrix_{timestamp}.png'))
        plt.close()
        
        # Log detailed metrics
        logging.info("\nModel Performance Metrics:")
        logging.info(f"Accuracy: {metrics['accuracy']:.4f}")
        logging.info(f"Precision: {metrics['precision']:.4f}")
        logging.info(f"Recall: {metrics['recall']:.4f}")
        logging.info(f"F1-Score: {metrics['f1_score']:.4f}")
        logging.info(f"\nDetailed metrics:\n{json.dumps(metrics, indent=2)}")
        
        return metrics

    def save_metrics(self, metrics: Dict, filepath: str) -> None:
        """
        Save evaluation metrics to file.
        
        Args:
            metrics: Dictionary of metrics
            filepath: Where to save the metrics
        """
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        with open(filepath, 'w') as f:
            json.dump(metrics, f, indent=2)
        logging.info(f"Metrics saved to {filepath}")

    def analyze_prediction(self, features: Dict, prediction: str, 
                         risk_score: float) -> Dict:
        """
        Analyze a single prediction in detail.
        
        Args:
            features: Input features
            prediction: Model prediction
            risk_score: Confidence score
            
        Returns:
            Dict containing analysis
        """
        # Identify most influential features
        feature_impacts = {}
        for feature_name, value in features.items():
            if isinstance(value, (int, float)):
                if value > 0:
                    feature_impacts[feature_name] = value
        
        # Sort features by impact
        sorted_features = sorted(feature_impacts.items(), 
                               key=lambda x: x[1], 
                               reverse=True)
        
        analysis = {
            'prediction': prediction,
            'risk_score': risk_score,
            'confidence': abs(risk_score - 0.5) * 2,  # Scale to 0-1
            'key_features': sorted_features[:3],  # Top 3 influential features
            'risk_level': self._determine_risk_level(risk_score)
        }
        
        return analysis
    
    def _determine_risk_level(self, risk_score: float) -> str:
        """
        Map risk score to category.
        
        Args:
            risk_score: Probability between 0 and 1
            
        Returns:
            Risk level category
        """
        if risk_score < 0.2:
            return "very_low"
        elif risk_score < 0.4:
            return "low"
        elif risk_score < 0.6:
            return "medium"
        elif risk_score < 0.8:
            return "high"
        else:
            return "very_high"