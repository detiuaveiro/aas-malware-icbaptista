from typing import Dict, List, Tuple
import re
import numpy as np
import torch
import torch.nn as nn
from abc import ABC, abstractmethod
from transformers import DistilBertTokenizer, DistilBERTModel
from ..features.feature_extractor import FeatureExtractor
from ..models.classifier import InjectionClassifier
from ..utils.constants import RISK_LEVELS

class PromptAnalyzer:
    """Handles detailed prompt analysis independent of classifier type"""
    def analyze_prompt_characteristics(self, prompt: str, features: Dict) -> Dict:
        analysis = {
            "length": len(prompt),
            "special_char_ratio": features["special_char_ratio"],
            "keyword_density": features["keyword_density"],
            "repetition_score": features["repetition_score"],
            "structure_complexity": features["structure_complexity"],
            "risk_factors": self._analyze_risk_factors(prompt, features),
            "pattern_matches": self._analyze_patterns(prompt)
        }
        
        # Add detailed linguistic analysis for DistilBERT
        if "bert_features" in features:
            analysis["semantic_analysis"] = {
                "attention_patterns": features["bert_features"]["attention"],
                "semantic_complexity": features["bert_features"]["complexity"]
            }
            
        return analysis
    
    def _analyze_patterns(self, prompt: str) -> Dict:
        """Analyze various patterns in the prompt"""
        return {
            "command_patterns": self._find_command_patterns(prompt),
            "manipulation_patterns": self._find_manipulation_patterns(prompt),
            "obfuscation_attempts": self._detect_obfuscation(prompt)
        }
    
    def _find_command_patterns(self, prompt: str) -> List[str]:
        command_patterns = [
            r"system\s*\(",
            r"exec\s*\(",
            r"eval\s*\(",
            r"shell\s*\(",
            r"\bsudo\b",
            r"\bchmod\b",
            r"\brm\s+-rf\b"
        ]
        return [pattern for pattern in command_patterns 
                if re.search(pattern, prompt, re.IGNORECASE)]
    
    def _find_manipulation_patterns(self, prompt: str) -> List[str]:
        manipulation_patterns = [
            r"ignore .*previous",
            r"forget .*training",
            r"you are now",
            r"new personality",
            r"override .*settings"
        ]
        return [pattern for pattern in manipulation_patterns 
                if re.search(pattern, prompt, re.IGNORECASE)]
    
    def _detect_obfuscation(self, prompt: str) -> Dict:
        """Detect potential obfuscation techniques"""
        return {
            "zero_width_spaces": "\u200b" in prompt,
            "unusual_unicode": bool(re.search(r'[^\x00-\x7F]', prompt)),
            "repeated_characters": bool(re.search(r'(.)\1{4,}', prompt)),
            "hidden_commands": bool(re.search(r'[`\'"].*[`\'"]', prompt))
        }
    
    def _analyze_risk_factors(self, prompt: str, features: Dict) -> List[Dict]:
        """Detailed analysis of risk factors"""
        risk_factors = []
        
        if features["has_system_commands"]:
            risk_factors.append({
                "type": "system_command",
                "severity": "high",
                "details": "Contains system-level command patterns",
                "matches": self._find_command_patterns(prompt)
            })
            
        if features["has_role_manipulation"]:
            risk_factors.append({
                "type": "role_manipulation",
                "severity": "high",
                "details": "Attempts to manipulate AI role/identity",
                "matches": self._find_manipulation_patterns(prompt)
            })
            
        obfuscation = self._detect_obfuscation(prompt)
        if any(obfuscation.values()):
            risk_factors.append({
                "type": "obfuscation",
                "severity": "medium",
                "details": "Potential obfuscation techniques detected",
                "techniques": obfuscation
            })
            
        return risk_factors

class BaseClassifier(ABC):
    @abstractmethod
    def predict(self, features: Dict) -> Tuple[float, str]:
        """
        Predict risk score and classification for given features.
        
        Args:
            features (Dict): Input features for classification
            
        Returns:
            Tuple[float, str]: (risk_score, prediction)
        """
        pass

    @abstractmethod
    def extract_analysis_features(self, prompt: str) -> Dict:
        """
        Extract model-specific analysis features.
        
        Args:
            prompt (str): Input prompt to analyze
            
        Returns:
            Dict: Extracted features
        """
        pass

class RandomForestClassifier(BaseClassifier):
    def __init__(self):
        self.model = InjectionClassifier()
    
    def predict(self, features: Dict) -> Tuple[float, str]:
        return self.model.predict(features)
        
    def extract_analysis_features(self, prompt: str) -> Dict:
        return {}  # RandomForest doesn't need special feature extraction
    
class DistilBERTClassifier(BaseClassifier):
    def __init__(self, model_name='distilbert-base-uncased'):
        self.tokenizer = DistilBertTokenizer.from_pretrained(model_name)
        self.model = DistilBERTModel()
    
    def predict(self, features: Dict) -> Tuple[float, str]:
        inputs = self.tokenizer(
            features['text'],
            return_tensors='pt',
            truncation=True,
            padding=True,
            max_length=128
        )
        
        with torch.no_grad():
            outputs = self.model(inputs)
            risk_score = outputs['prediction'].item()
            prediction = "malicious" if risk_score > 0.5 else "benign"
            
            features['bert_features'] = {
                'attention': outputs['attention_patterns'],
                'complexity': outputs['semantic_complexity']
            }
            
        return risk_score, prediction
        
    def extract_analysis_features(self, prompt: str) -> Dict:
        """Extract BERT-specific features"""
        inputs = self.tokenizer(
            prompt,
            return_tensors='pt',
            truncation=True,
            padding=True,
            max_length=128
        )
        
        with torch.no_grad():
            outputs = self.model(inputs)
            return {
                'attention_patterns': outputs['attention_patterns'],
                'semantic_complexity': outputs['semantic_complexity']
            }

class InjectionDetector:
    def __init__(self, classifier_type="randomforest"):
        self.feature_extractor = FeatureExtractor()
        self.prompt_analyzer = PromptAnalyzer()
        if classifier_type == "randomforest":
            self.classifier = RandomForestClassifier()
        elif classifier_type == "distilbert":
            self.classifier = DistilBERTClassifier()
        else:
            raise ValueError("Unsupported classifier type")
        
    def analyze_prompt(self, prompt: str) -> Dict:
        """
        Analyze a single prompt for potential injection attacks.
        """
        # Extract features
        features = self.feature_extractor.extract_features(prompt)
        features['text'] = prompt  # Add raw text for BERT model
        
        # Get classification and confidence
        risk_score, prediction = self.classifier.predict(features)
        
        # Get detailed analysis
        analysis = self.prompt_analyzer.analyze_prompt_characteristics(prompt, features)
        
        return {
            "prompt": prompt,
            "risk_score": risk_score,
            "prediction": prediction,
            "risk_level": self._determine_risk_level(risk_score),
            "analysis": analysis,
            "features": features
        }
    
    def _determine_risk_level(self, risk_score: float) -> str:
        """
        Convert risk score to risk level category.
        """
        for level, threshold in RISK_LEVELS.items():
            if risk_score <= threshold:
                return level
        return "critical"