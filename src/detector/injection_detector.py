"""
Prompt injection detection and analysis system.
Includes multiple classifiers and detailed analysis capabilities.
"""

from typing import Dict, List, Tuple
import re
import logging
import numpy as np
import torch
import torch.nn as nn
from datetime import datetime
from abc import ABC, abstractmethod
from transformers import DistilBertModel, DistilBertTokenizer
from ..features.feature_extractor import FeatureExtractor
from ..models.classifier import InjectionClassifier
from ..utils.constants import RISK_LEVELS

class PromptAnalyzer:
    """Analyzes prompt characteristics and patterns"""
    
    def analyze_prompt_characteristics(self, prompt: str, features: Dict) -> Dict:
        """
        Perform comprehensive prompt analysis.
        
        Args:
            prompt (str): Input text
            features (Dict): Extracted features
            
        Returns:
            Dict: Analysis results including length, ratios, and risk factors
        """
        logging.info(f"Analyzing prompt characteristics for text length: {len(prompt)}")
        
        analysis = {
            "length": len(prompt),
            "special_char_ratio": features["special_char_ratio"],
            "keyword_density": features["keyword_density"],
            "repetition_score": features["repetition_score"],
            "structure_complexity": features["structure_complexity"],
            "risk_factors": self._analyze_risk_factors(prompt, features),
            "pattern_matches": self._analyze_patterns(prompt)
        }
        
        if "bert_features" in features:
            logging.info("Adding BERT-specific analysis")
            analysis["semantic_analysis"] = {
                "attention_patterns": features["bert_features"]["attention"],
                "semantic_complexity": features["bert_features"]["complexity"]
            }
            
        logging.info("Prompt characteristics analysis completed")
        return analysis
    
    def _analyze_patterns(self, prompt: str) -> Dict:
        """
        Find suspicious patterns in text.
        
        Args:
            prompt (str): Input text
            
        Returns:
            Dict: Detected patterns by category
        """
        logging.info("Analyzing prompt patterns")
        patterns = {
            "command_patterns": self._find_command_patterns(prompt),
            "manipulation_patterns": self._find_manipulation_patterns(prompt),
            "obfuscation_attempts": self._detect_obfuscation(prompt)
        }
        logging.info(f"Found {sum(len(p) if isinstance(p, list) else 1 for p in patterns.values())} total patterns")
        return patterns
    
    def _find_command_patterns(self, prompt: str) -> List[str]:
        """
        Detect system command patterns.
        
        Args:
            prompt (str): Input text
            
        Returns:
            List[str]: Matched command patterns
        """
        logging.info("Checking for command patterns")
        command_patterns = [
            r"system\s*\(",
            r"exec\s*\(",
            r"eval\s*\(",
            r"shell\s*\(",
            r"\bsudo\b",
            r"\bchmod\b",
            r"\brm\s+-rf\b"
        ]
        matches = [pattern for pattern in command_patterns 
                if re.search(pattern, prompt, re.IGNORECASE)]
        logging.info(f"Found {len(matches)} command patterns")
        return matches
    
    def _find_manipulation_patterns(self, prompt: str) -> List[str]:
        """
        Detect AI manipulation attempts.
        
        Args:
            prompt (str): Input text
            
        Returns:
            List[str]: Matched manipulation patterns
        """
        logging.info("Checking for manipulation patterns")
        manipulation_patterns = [
            r"ignore .*previous",
            r"forget .*training",
            r"you are now",
            r"new personality",
            r"override .*settings"
        ]
        matches = [pattern for pattern in manipulation_patterns 
                if re.search(pattern, prompt, re.IGNORECASE)]
        logging.info(f"Found {len(matches)} manipulation patterns")
        return matches
    
    def _detect_obfuscation(self, prompt: str) -> Dict:
        """
        Detect text obfuscation attempts.
        
        Args:
            prompt (str): Input text
            
        Returns:
            Dict: Obfuscation techniques found
        """
        logging.info("Checking for obfuscation techniques")
        techniques = {
            "zero_width_spaces": "\u200b" in prompt,
            "unusual_unicode": bool(re.search(r'[^\x00-\x7F]', prompt)),
            "repeated_characters": bool(re.search(r'(.)\1{4,}', prompt)),
            "hidden_commands": bool(re.search(r'[`\'"].*[`\'"]', prompt))
        }
        logging.info(f"Found {sum(techniques.values())} obfuscation techniques")
        return techniques
    
    def _analyze_risk_factors(self, prompt: str, features: Dict) -> List[Dict]:
        """
        Analyze security risk factors.
        
        Args:
            prompt (str): Input text
            features (Dict): Extracted features
            
        Returns:
            List[Dict]: Risk factors with severity and details
        """
        logging.info("Analyzing risk factors")
        risk_factors = []
        
        if features["has_system_commands"]:
            risk_factors.append({
                "type": "system_command",
                "severity": "high",
                "details": "Contains system-level command patterns",
                "matches": self._find_command_patterns(prompt)
            })
            
        if features["has_role_manipulation"]:
            risk_factors.append({
                "type": "role_manipulation",
                "severity": "high",
                "details": "Attempts to manipulate AI role/identity",
                "matches": self._find_manipulation_patterns(prompt)
            })
            
        obfuscation = self._detect_obfuscation(prompt)
        if any(obfuscation.values()):
            risk_factors.append({
                "type": "obfuscation",
                "severity": "medium",
                "details": "Potential obfuscation techniques detected",
                "techniques": obfuscation
            })
            
        logging.info(f"Found {len(risk_factors)} risk factors")
        return risk_factors

class BaseClassifier(ABC):
    """Base class for injection classifiers"""
    
    @abstractmethod
    def predict(self, features: Dict) -> Tuple[float, str]:
        """Abstract prediction method"""
        pass

    @abstractmethod
    def extract_analysis_features(self, prompt: str) -> Dict:
        """Abstract feature extraction method"""
        pass

class RandomForestClassifier(BaseClassifier):
    """Random Forest-based classifier implementation"""
    
    def __init__(self):
        logging.info("Initializing Random Forest classifier")
        self.model = InjectionClassifier()
    
    def predict(self, features: Dict) -> Tuple[float, str]:
        """
        Predict using Random Forest model.
        
        Args:
            features (Dict): Input features
            
        Returns:
            Tuple[float, str]: (risk_score, prediction)
        """
        logging.info("Making Random Forest prediction")
        return self.model.predict(features)
        
    def extract_analysis_features(self, prompt: str) -> Dict:
        """No special features needed for Random Forest"""
        return {}

class DistilBERTClassifier(BaseClassifier):
    """DistilBERT-based classifier implementation"""
    
    def __init__(self, model_name='distilbert-base-uncased'):
        logging.info(f"Initializing DistilBERT classifier: {model_name}")
        self.tokenizer = DistilBertTokenizer.from_pretrained(model_name)
        self.model = DistilBertModel()
    
    def predict(self, features: Dict) -> Tuple[float, str]:
        """
        Predict using DistilBERT model.
        
        Args:
            features (Dict): Input features with text
            
        Returns:
            Tuple[float, str]: (risk_score, prediction)
        """
        logging.info("Tokenizing input for DistilBERT")
        inputs = self.tokenizer(
            features['text'],
            return_tensors='pt',
            truncation=True,
            padding=True,
            max_length=128
        )
        
        logging.info("Making DistilBERT prediction")
        with torch.no_grad():
            outputs = self.model(inputs)
            risk_score = outputs['prediction'].item()
            prediction = "malicious" if risk_score > 0.5 else "benign"
            
            features['bert_features'] = {
                'attention': outputs['attention_patterns'],
                'complexity': outputs['semantic_complexity']
            }
            
        logging.info(f"DistilBERT prediction: {prediction} (score: {risk_score:.4f})")
        return risk_score, prediction
        
    def extract_analysis_features(self, prompt: str) -> Dict:
        """
        Extract BERT-specific features.
        
        Args:
            prompt (str): Input text
            
        Returns:
            Dict: BERT attention and complexity features
        """
        logging.info("Extracting BERT-specific features")
        inputs = self.tokenizer(
            prompt,
            return_tensors='pt',
            truncation=True,
            padding=True,
            max_length=128
        )
        
        with torch.no_grad():
            outputs = self.model(inputs)
            features = {
                'attention_patterns': outputs['attention_patterns'],
                'semantic_complexity': outputs['semantic_complexity']
            }
        logging.info("BERT feature extraction completed")
        return features

class InjectionDetector:
    """Main class for prompt injection detection"""
    
    def __init__(self, classifier_type="randomforest"):
        """
        Initialize detector with specified classifier.
        
        Args:
            classifier_type (str): "randomforest" or "distilbert"
        """
        logging.info(f"Initializing InjectionDetector with {classifier_type} classifier")
        self.feature_extractor = FeatureExtractor()
        self.prompt_analyzer = PromptAnalyzer()
        
        if classifier_type == "randomforest":
            self.classifier = RandomForestClassifier()
        elif classifier_type == "distilbert":
            self.classifier = DistilBERTClassifier()
        else:
            raise ValueError("Unsupported classifier type")
        
    def analyze_prompt(self, prompt: str) -> Dict:
        """
        Analyze prompt for injection attempts.
        
        Args:
            prompt (str): Input text
            
        Returns:
            Dict: Complete analysis including:
                - risk_score: Injection probability
                - prediction: "malicious" or "benign"
                - risk_level: Risk category
                - analysis: Detailed characteristics
                - features: Extracted features
        """
        logging.info(f"\nAnalyzing prompt: {prompt[:50]}...")
        
        # Extract features
        features = self.feature_extractor.extract_features(prompt)
        features['text'] = prompt
        
        # Get classification
        risk_score, prediction = self.classifier.predict(features)
        logging.info(f"Classification complete - Risk score: {risk_score:.4f}")
        
        # Get detailed analysis
        analysis = self.prompt_analyzer.analyze_prompt_characteristics(prompt, features)
        
        result = {
            "prompt": prompt,
            "risk_score": risk_score,
            "prediction": prediction,
            "risk_level": self._determine_risk_level(risk_score),
            "analysis": analysis,
            "features": features
        }
        logging.info("Analysis complete")
        return result
    
    def _determine_risk_level(self, risk_score: float) -> str:
        """
        Map risk score to category.
        
        Args:
            risk_score (float): Risk probability
            
        Returns:
            str: Risk level category
        """
        for level, threshold in RISK_LEVELS.items():
            if risk_score <= threshold:
                return level
        return "critical"